{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c13818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\assaf\\Documents\\code\\neural\\final\\tf-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q kagglehub\n",
    "import kagglehub\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Download latest version of the MoveNet Thunder model\n",
    "model_dir = kagglehub.model_download(\"google/movenet/tensorFlow2/singlepose-thunder\")\n",
    "# model_dir will be something like:\n",
    "#   \"C:/Users/YourUserName/.cache/kagglehub/models/google/movenet/tensorFlow2/singlepose-thunder\"\n",
    "\n",
    "# Sometimes MoveNet model files are stored under a subdirectory named \"4\"\n",
    "# Check if \"4\" exists in model_dir; if so, point model_path to that subdirectory:\n",
    "model_path = os.path.join(model_dir, \"4\")\n",
    "if not os.path.exists(model_path):\n",
    "    # If there's no '4' subfolder, we can just use model_dir directly:\n",
    "    model_path = model_dir\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.saved_model.load(model_path)\n",
    "movenet = model.signatures['serving_default']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee255da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./position_files\\good_right\\63009_4.npy\n",
      "Saved: ./position_files\\good_right\\63043_1.npy\n",
      "Saved: ./position_files\\good_right\\63094_1.npy\n",
      "Saved: ./position_files\\good_right\\63178_1.npy\n",
      "Saved: ./position_files\\good_right\\63208_2.npy\n",
      "Saved: ./position_files\\good_right\\63240_2.npy\n",
      "Saved: ./position_files\\good_right\\63241_1.npy\n",
      "Saved: ./position_files\\good_right\\63274_5.npy\n",
      "Saved: ./position_files\\good_right\\63538_1.npy\n",
      "Saved: ./position_files\\good_right\\63687_6.npy\n",
      "Saved: ./position_files\\good_right\\63692_5.npy\n",
      "Saved: ./position_files\\good_right\\79818_1.npy\n",
      "Saved: ./position_files\\good_right\\79863_4.npy\n",
      "Saved: ./position_files\\good_right\\80125_2.npy\n",
      "Saved: ./position_files\\good_right\\80282_2.npy\n",
      "Saved: ./position_files\\good_right\\80368_1.npy\n",
      "Saved: ./position_files\\good_right\\80434_1.npy\n",
      "Saved: ./position_files\\good_right\\80532_1.npy\n",
      "Saved: ./position_files\\good_right\\80561_3.npy\n",
      "Saved: ./position_files\\good_right\\80638_4.npy\n",
      "Saved: ./position_files\\good_right\\80661_3.npy\n",
      "Saved: ./position_files\\good_right\\80672_2.npy\n",
      "Saved: ./position_files\\good_right\\80713_7.npy\n",
      "Saved: ./position_files\\good_right\\80889_1.npy\n",
      "Saved: ./position_files\\good_right\\80891_1.npy\n",
      "Saved: ./position_files\\good_right\\80897_5.npy\n",
      "Saved: ./position_files\\good_right\\80914_2.npy\n",
      "Saved: ./position_files\\good_right\\flipped_62945_2.npy\n",
      "Saved: ./position_files\\good_right\\flipped_63016_1.npy\n",
      "Saved: ./position_files\\good_right\\flipped_63175_4.npy\n",
      "Saved: ./position_files\\good_right\\flipped_63207_1.npy\n",
      "Saved: ./position_files\\good_right\\flipped_63291_2.npy\n",
      "Saved: ./position_files\\good_right\\flipped_63357_3.npy\n",
      "Saved: ./position_files\\good_right\\flipped_63560_3.npy\n",
      "Saved: ./position_files\\good_right\\flipped_79716_6.npy\n",
      "Saved: ./position_files\\good_right\\flipped_79897_12.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80091_3.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80115_3.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80152_1.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80170_3.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80351_3.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80453_1.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80520_1.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80557_5.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80621_2.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80892_2.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80902_5.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80905_1.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80909_3.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80919_5.npy\n",
      "Saved: ./position_files\\good_right\\flipped_80922_2.npy\n",
      "Saved: ./position_files\\wide_right\\63090_3.npy\n",
      "Saved: ./position_files\\wide_right\\63097_1.npy\n",
      "Saved: ./position_files\\wide_right\\63156_1.npy\n",
      "Saved: ./position_files\\wide_right\\63296_2.npy\n",
      "Saved: ./position_files\\wide_right\\63474_2.npy\n",
      "Saved: ./position_files\\wide_right\\63729_5.npy\n",
      "Saved: ./position_files\\wide_right\\80467_3.npy\n",
      "Saved: ./position_files\\wide_right\\80898_1.npy\n",
      "Saved: ./position_files\\wide_right\\80921_1.npy\n",
      "Saved: ./position_files\\wide_right\\flipped_63262_1.npy\n",
      "Saved: ./position_files\\wide_right\\flipped_63309_8.npy\n",
      "Saved: ./position_files\\wide_right\\flipped_63738_1.npy\n",
      "Saved: ./position_files\\wide_right\\flipped_63815_1.npy\n",
      "Saved: ./position_files\\wide_right\\flipped_79912_2.npy\n",
      "Saved: ./position_files\\wide_right\\flipped_80183_4.npy\n",
      "Saved: ./position_files\\wide_right\\flipped_80627_3.npy\n",
      "Saved: ./position_files\\wide_right\\flipped_80685_1.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(frame, axis=0), 256, 256)\n",
    "    img = tf.cast(img, dtype=tf.int32)\n",
    "    return img\n",
    "\n",
    "def extract_keypoints(frame):\n",
    "    input_image = preprocess_frame(frame)\n",
    "    outputs = movenet(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    flat_input = keypoints[0, 0, :, :2].flatten()  # (34,)\n",
    "    return flat_input\n",
    "\n",
    "# Set paths\n",
    "video_root_dir = './videos'\n",
    "output_root_dir = './position_files'\n",
    "os.makedirs(output_root_dir, exist_ok=True)\n",
    "\n",
    "# Walk through label subdirectories\n",
    "for label in os.listdir(video_root_dir):\n",
    "    label_video_dir = os.path.join(video_root_dir, label)\n",
    "    if not os.path.isdir(label_video_dir):\n",
    "        continue  # Skip files\n",
    "\n",
    "    # Create matching output subdirectory\n",
    "    label_output_dir = os.path.join(output_root_dir, label)\n",
    "    os.makedirs(label_output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each video in that subdirectory\n",
    "    for filename in os.listdir(label_video_dir):\n",
    "        if filename.endswith('.mp4'):\n",
    "            video_path = os.path.join(label_video_dir, filename)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "            data = []\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                flat_input = extract_keypoints(rgb)\n",
    "                data.append(flat_input)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "            data = np.array(data)  # (num_frames, 34)\n",
    "            output_path = os.path.join(label_output_dir, f\"{os.path.splitext(filename)[0]}.npy\")\n",
    "            np.save(output_path, data)\n",
    "            print(f\"Saved: {output_path}\")\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "input_dir = './position_files'\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# Read each labeled subdirectory\n",
    "for label in os.listdir(input_dir):\n",
    "    label_path = os.path.join(input_dir, label)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    for fname in os.listdir(label_path):\n",
    "        if fname.endswith('.npy'):\n",
    "            path = os.path.join(label_path, fname)\n",
    "            data = np.load(path)  # shape: (frames, 34)\n",
    "            X_list.append(data)\n",
    "            y_list.append(label)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "X_padded = pad_sequences(X_list, padding='post', dtype='float32')  # (num_samples, max_timesteps, 34)\n",
    "\n",
    "# Encode labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_list)  # e.g., \"good_form\" â†’ 0, etc.\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X shape:\", X_train.shape)\n",
    "print(\"y shape:\", y_train.shape)\n",
    "print(\"Classes:\", label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, BatchNormalization\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "features = X_train.shape[2]\n",
    "num_classes = 3  # you said 3 outputs\n",
    "\n",
    "model = Sequential([\n",
    "    Masking(mask_value=0.0, input_shape=(timesteps, features)),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "model.save(\"ohp_classifier_lstm.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
